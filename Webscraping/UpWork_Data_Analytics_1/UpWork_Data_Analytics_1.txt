import requests
from bs4 import BeautifulSoup

#title, price, availability, size, dispatch time uk, dispatch time intl, product type, signed by, presentation type,

def scrape_product_details(url):
    response = requests.get(url)
    if response.status_code != 200:
        return {'Title': 'Failed to retrieve', 'Price': 'Failed to retrieve'}

    soup = BeautifulSoup(response.content, 'html.parser')

    title = soup.find('h1').text.strip() if soup.find('h1') else 'Title not found'
    price_container = soup.find('div', class_='product-info-price')
    price = price_container.find('span', class_='price').text.strip() if price_container and price_container.find('span', class_='price') else 'Price not found'

    return {'Title': title, 'Price': price}

def scrape_product_size(url):
    response = requests.get(url)
    if response.status_code != 200:
        return 'Size information retrieval failed'

    soup = BeautifulSoup(response.content, 'html.parser')
    size_container = soup.find('div', class_='product size')
    if size_container:
        strong_tag = size_container.find('strong')
        if strong_tag:
            strong_tag.decompose()
        size = size_container.text.strip()
    else:
        size = 'Size information not found'

    return size

def extract_dispatch_time_uk(url):
    response = requests.get(url)
    if response.status_code != 200:
        return 'Dispatch time UK retrieval failed'

    soup = BeautifulSoup(response.content, 'html.parser')
    data_item_content = soup.find('div', class_='data item content', id='custom_1604277903302_302')
    if not data_item_content:
        return 'Dispatch time UK not found'

    courier_details_p = data_item_content.find('p', string=lambda text: "Courier Details – UK" in text)
    if not courier_details_p:
        return 'Dispatch time UK not found'

    next_p = courier_details_p.find_next_sibling('p')
    if not next_p:
        return 'Dispatch time UK not found'

    text_parts = next_p.text.split()
    if 'within' in text_parts:
        within_index = text_parts.index('within')
        if within_index + 1 < len(text_parts):
            dispatch_time_word = text_parts[within_index + 1]
            mapping = {'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5, 'six': 6, 'seven': 7, 'eight': 8, 'nine': 9}
            return mapping.get(dispatch_time_word.lower(), 'Dispatch time word not a number')
    else:
        return 'Dispatch time UK not found'

def extract_dispatch_time_international(url):
    response = requests.get(url)
    if response.status_code != 200:
        return {"International": "Dispatch time International retrieval failed"}

    soup = BeautifulSoup(response.content, 'html.parser')
    data_item_content = soup.find('div', class_='data item content', id='custom_1604277903302_302')
    if not data_item_content:
        return {"International": "Dispatch time International not found"}

    section_p = data_item_content.find('p', string=lambda text: "Courier Details – International (non-UK)" in text)
    if section_p:
        next_p = section_p.find_next_sibling('p')
        if next_p:
            text_parts = next_p.text.split()
            if 'within' in text_parts:
                within_index = text_parts.index('within')
                if within_index + 1 < len(text_parts):
                    dispatch_time_word = text_parts[within_index + 1]
                    try:
                        return {"International": int(dispatch_time_word)}
                    except ValueError:
                        return {"International": f"'{dispatch_time_word}' is not a number."}
            else:
                return {"International": "Dispatch time International not found"}
        else:
            return {"International": "Next p element after 'Courier Details – International (non-UK)' not found"}
    else:
        return {"International": "p element with 'Courier Details – International (non-UK)' not found"}

    return {"International": "Dispatch time International not found"}



def extract_product_type(url):
    response = requests.get(url)
    if response.status_code != 200:
        return 'Product type retrieval failed'

    soup = BeautifulSoup(response.content, 'html.parser')
    data_item_content = soup.find('div', class_='data item content', id='additional')
    if not data_item_content:
        return 'Product type not found'

    specs_table = data_item_content.find('table', id='product-attribute-specs-table')
    if specs_table:
        for tr in specs_table.find_all('tr'):
            th = tr.find('th')
            if th and 'Product type' in th.get_text():
                td = tr.find('td')
                return td.get_text().strip() if td else 'Product type not found'
    return 'Product type not found'

def extract_signed_by(url):
    response = requests.get(url)
    if response.status_code != 200:
        return 'Signed by retrieval failed'

    soup = BeautifulSoup(response.content, 'html.parser')
    data_item_content = soup.find('div', class_='data item content', id='additional')
    if not data_item_content:
        return 'Signed by not found'

    specs_table = data_item_content.find('table', id='product-attribute-specs-table')
    if specs_table:
        for tr in specs_table.find_all('tr'):
            th = tr.find('th')
            if th and 'Signed by' in th.get_text():
                td = tr.find('td')
                return td.get_text().strip() if td else 'Signed by not found'
    return 'Signed by not found'

def extract_presentation_type(url):
    response = requests.get(url)
    if response.status_code != 200:
        return 'Presentation type retrieval failed'

    soup = BeautifulSoup(response.content, 'html.parser')
    data_item_content = soup.find('div', class_='data item content', id='additional')
    if not data_item_content:
        return 'Presentation type not found'

    specs_table = data_item_content.find('table', id='product-attribute-specs-table')
    if specs_table:
        for tr in specs_table.find_all('tr'):
            th = tr.find('th')
            if th and 'Presentation type' in th.get_text():
                td = tr.find('td')
                return td.get_text().strip() if td else 'Presentation type not found'
    return 'Presentation type not found'

def check_add_to_cart_presence(url):
    response = requests.get(url)
    if response.status_code != 200:
        return 'Check failed'

    soup = BeautifulSoup(response.content, 'html.parser')
    add_to_cart_link = soup.find('a', class_='action tocart primary', string='Add to Cart')
    return 'Available' if add_to_cart_link else 'Not Available'

# URL of the product
product_url = "https://www.icons.com/ronaldo-official-uefa-champions-league-back-signed-and-framed-modern-real-madrid-cf-home-shirt-with-fan-style-numbers-37256.html"

#scrape_product_details(product_url)['Title']
#scrape_product_details(product_url)['Price']
#scrape_product_size(product_url)
#extract_dispatch_time_uk(product_url)
#extract_dispatch_time_international(product_url)
#extract_product_type(product_url)
#extract_signed_by(product_url)
#extract_presentation_type(product_url)
#check_add_to_cart_presence(product_url)

import requests
from bs4 import BeautifulSoup
import pandas as pd

page_url = "https://www.icons.com/players/a-k.html"

def extract_player_names_and_links(url):
    response = requests.get(url)
    if response.status_code != 200:
        print("Failed to retrieve the webpage")
        return pd.DataFrame()  # Return empty DataFrame in case of failure

    soup = BeautifulSoup(response.content, 'html.parser')

    player_divs = soup.find_all('div', style="padding-bottom:10px")
    player_info = []

    for div in player_divs:
        a_tag = div.find('a')
        if a_tag and 'href' in a_tag.attrs:
            player_name = a_tag.get_text().strip()
            if player_name[0].upper() in ['A', 'B', 'C']:
                player_link = a_tag['href']
                player_info.append({'Name': player_name, 'Link': player_link})

    return pd.DataFrame(player_info)


def extract_product_urls_from_page(url):
    response = requests.get(url)
    if response.status_code != 200:
        print("Failed to retrieve the webpage")
        return []

    soup = BeautifulSoup(response.content, 'html.parser')

    # Assuming that product links are 'a' tags with class 'product photo product-item-photo'
    product_links = soup.find_all('a', class_='product photo product-item-photo')

    # Extract 'href' attribute from each link
    product_urls = [link.get('href') for link in product_links if link.get('href')]

    return product_urls

player_df = extract_player_names_and_links(page_url)
player_df['Product URLs'] = player_df['Link'].apply(extract_product_urls_from_page)
exploded_df = player_df.explode('Product URLs')
exploded_df = exploded_df.reset_index(drop=True)


def extract_all_details(url):
    details = {
        "Title": scrape_product_details(url)["Title"],
        "Price": scrape_product_details(url)["Price"],
        "Size": scrape_product_size(url),
        "Dispatch Time UK": extract_dispatch_time_uk(url),
        "Dispatch Time International": extract_dispatch_time_international(url)["International"],
        "Product Type": extract_product_type(url),
        "Signed By": extract_signed_by(url),
        "Presentation Type": extract_presentation_type(url),
        "Availability": check_add_to_cart_presence(url)
    }
    return details

final_df = pd.DataFrame()

# Iterate over each row in exploded_df and get details for each product URL
for index, row in exploded_df.iterrows():
    product_details = extract_all_details(row['Product URLs'])
    product_details['Name'] = row['Name']
    product_details['Link'] = row['Link']
    product_details['Product URLs'] = row['Product URLs']
    final_df = final_df.append(product_details, ignore_index=True)

# Reorder columns if needed
final_df = final_df[['Name', 'Link', 'Product URLs', 'Title', 'Price', 'Availability', 'Size', 'Dispatch Time UK', 'Dispatch Time International', 'Product Type', 'Signed By', 'Presentation Type']]

# Reset index if required
final_df.reset_index(drop=True, inplace=True)

final_df

final_df.to_csv('C:\\Users\\ASUS\\Desktop\\player_product_details.csv', index=False)
